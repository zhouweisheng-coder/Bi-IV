[TOC]

# 第七章 神经网络基础与移动推荐系统

## Thinking1：什么是反向传播中的链式法则

* 神经网络一层一层地前向传播得到最终误差，再通过输出层的误差反向一层一层的传播得到每一层的误差，以此更新权重w和b

## Thinking2：请列举几种常见的激活函数，激活函数有什么作用

* ReLU、sigmoid、tanh、softmax
* 激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。引入非线性激活函数，可使得深层神经网络的表达能力更加强大。

## Thinking3：利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？

* 可能是梯度消失问题、数据处理不当
* 针对梯度消失问题的解决办法：
  * 通过调整学习率
  * 减少神经网络层数
  * 更换合适的激活函数
  * 改变初始的权值