[TOC]

# 第七章 神经网络基础与移动推荐系统

## Thinking1：什么是反向传播中的链式法则

* 链式法则是微积分中的求导法则，用于求一个复合函数的导数，在神经网络中，可以通过链式法则，把求导计算拆解成很多个部分，可以随着反向传播逐层计算。

## Thinking2：请列举几种常见的激活函数，激活函数有什么作用

* ReLU、sigmoid、tanh、softmax
* 激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。引入非线性激活函数，可使得深层神经网络的表达能力更加强大。

## Thinking3：利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？

* 可能是梯度消失问题、数据处理不当
* 针对梯度消失问题的解决办法：
  * 通过调整学习率
  * 减少神经网络层数
  * 更换合适的激活函数
  * 改变初始的权值
